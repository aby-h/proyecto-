{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "class Clasificadores:\n",
        "    def __init__(self):\n",
        "        self.regresion_logistica = LogisticRegression(max_iter=1000)\n",
        "        self.knn = KNeighborsClassifier()\n",
        "        self.svm = SVC()\n",
        "        self.arbol_decision = DecisionTreeClassifier()\n",
        "        self.bosque_aleatorio = RandomForestClassifier()\n",
        "\n",
        "    def entrenar_modelo(self, modelo, datos_entrenamiento, etiquetas_entrenamiento):\n",
        "        try:\n",
        "            modelo.fit(datos_entrenamiento, etiquetas_entrenamiento)\n",
        "        except Exception as e:\n",
        "            print(f\"Error al entrenar el modelo {modelo}: {e}\")\n",
        "\n",
        "    def generar_predicciones(self, modelo, datos_prueba):\n",
        "        try:\n",
        "            return modelo.predict(datos_prueba)\n",
        "        except Exception as e:\n",
        "            print(f\"Error al predecir con el modelo {modelo}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def evaluar_modelo(self, etiquetas_reales, etiquetas_predichas):\n",
        "        print(\"\\nMatriz de Confusión:\")\n",
        "        print(confusion_matrix(etiquetas_reales, etiquetas_predichas))\n",
        "        print(\"\\nReporte de Clasificación:\")\n",
        "        print(classification_report(etiquetas_reales, etiquetas_predichas))\n",
        "        print(f\"Precisión: {accuracy_score(etiquetas_reales, etiquetas_predichas):.2f}\")\n",
        "\n",
        "# Definición del bloque principal para ejecutar el script\n",
        "def main():\n",
        "    # Lectura de datos desde archivo\n",
        "    ruta_datos = \"datos.xlsx\"\n",
        "    try:\n",
        "        dataset = pd.read_excel(ruta_datos)\n",
        "    except FileNotFoundError:\n",
        "        print(\"El archivo de datos no se encontró.\")\n",
        "        return\n",
        "\n",
        "    # Visualización inicial de los datos\n",
        "    print(\"Primeras filas del conjunto de datos:\")\n",
        "    print(dataset.head())\n",
        "\n",
        "    # Separación en características (X) y etiquetas (y)\n",
        "    caracteristicas = dataset.iloc[:, 2:]  # Columnas que contienen las características\n",
        "    etiquetas = dataset.iloc[:, 1]         # Columna con las etiquetas\n",
        "\n",
        "    # División en datos de entrenamiento y prueba\n",
        "    X_entrenamiento, X_prueba, y_entrenamiento, y_prueba = train_test_split(caracteristicas, etiquetas, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Instancia de la clase de clasificadores\n",
        "    clasificadores = Clasificadores()\n",
        "\n",
        "    # Entrenamiento y evaluación de cada modelo\n",
        "    for nombre_modelo, modelo in clasificadores.__dict__.items():\n",
        "        print(f\"\\nEntrenando el modelo: {nombre_modelo}...\")\n",
        "        clasificadores.entrenar_modelo(modelo, X_entrenamiento, y_entrenamiento)\n",
        "\n",
        "        print(f\"Evaluando el modelo: {nombre_modelo}...\")\n",
        "        predicciones = clasificadores.generar_predicciones(modelo, X_prueba)\n",
        "        if predicciones is not None:\n",
        "            clasificadores.evaluar_modelo(y_prueba, predicciones)\n",
        "\n",
        "    # Comparación visual de la precisión promedio de cada modelo\n",
        "    resultados_validacion = {}\n",
        "    for nombre_modelo, modelo in clasificadores.__dict__.items():\n",
        "        try:\n",
        "            puntuaciones = cross_val_score(modelo, caracteristicas, etiquetas, cv=5, scoring='accuracy')\n",
        "            resultados_validacion[nombre_modelo] = puntuaciones.mean()\n",
        "        except Exception as e:\n",
        "            print(f\"Error durante la validación cruzada del modelo {nombre_modelo}: {e}\")\n",
        "\n",
        "    # Gráfica comparativa de precisión\n",
        "    nombres_modelos = list(resultados_validacion.keys())\n",
        "    valores_precision = list(resultados_validacion.values())\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=valores_precision, y=nombres_modelos, palette=\"viridis\")\n",
        "    plt.xlabel(\"Precisión Promedio (Validación Cruzada)\")\n",
        "    plt.ylabel(\"Modelo\")\n",
        "    plt.title(\"Comparación de Precisión entre Modelos\")\n",
        "    plt.show()\n",
        "\n",
        "# Verificación de ejecución directa\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ER-0acPJRKMH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}